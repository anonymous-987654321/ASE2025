{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_names = ['bmcsoftware_vscode-ispw', 'IBM_example-health-apis',\n",
    "       'debinix_openjensen', 'Martinfx_Cobol',\n",
    "       'shamrice_COBOL-RSS-Reader', 'shamrice_COBOL-Roguelike',\n",
    "       'shamrice_COBOL-Guest-Book-Webapp', 'ibmdbbdev_Samples',\n",
    "       'brazilofmux_gnucobol',\n",
    "       'cicsdev_cics-async-api-credit-card-application-example',\n",
    "       'cicsdev_cics-async-api-redbooks', 'walmartlabs_zECS',\n",
    "       'cicsdev_cics-banking-sample-application-cbsa',\n",
    "       'cicsdev_cics-genapp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(),'..','data', 'projects')\n",
    "repo_clean_dir = os.path.join(os.getcwd(),'..','data', 'project_clean')\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_reference_df = pd.read_csv(os.path.join('..','data','file_level_reference_dataset.csv'), index_col=0)\n",
    "file_reference_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get all cbl files from all projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = subprocess.run([\"find\", data_dir, \"-type\", \"f\", \"-name\", \"*.cbl\"], capture_output=True, text=True, check=True)\n",
    "file_paths = result.stdout.splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### copy COBOL files in /projects and seperate multiprograms, remove comments and empty lines, write to /project_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect the length of the numeric prefix\n",
    "def detect_number_length(s):\n",
    "    match = re.match(r'^\\d+', s)\n",
    "    if match:\n",
    "        return len(match.group())\n",
    "    return 0\n",
    "\n",
    "# Function to check if all strings start with a number of the detected length\n",
    "def all_start_with_detected_length_number(strings):\n",
    "    # Find the first string with a numeric prefix to detect the length\n",
    "    number_length = 0\n",
    "    for string in strings:\n",
    "        number_length = detect_number_length(string)\n",
    "        if number_length > 0:\n",
    "            break\n",
    "\n",
    "    if number_length == 0:\n",
    "        return False\n",
    "\n",
    "    # Define the regular expression to match a number of the detected length at the beginning of the string\n",
    "    pattern = re.compile(rf'^\\d{{{number_length}}}')\n",
    "\n",
    "    # Check if all strings that start with a number have the detected length\n",
    "    return all(pattern.match(string) or not re.match(r'^\\d+', string) for string in strings)\n",
    "\n",
    "\n",
    "def replace_numeric_edges_with_space(s):\n",
    "    #if not s.startswith(' ') and not s.startswith('*'):\n",
    "    if not s.startswith('*'):\n",
    "        # Replace numeric string at the beginning with corresponding spaces\n",
    "        s = re.sub(r'^\\d+', lambda match: ' ' * len(match.group()), s)\n",
    "        # Replace numeric string at the end with corresponding spaces\n",
    "        s = re.sub(r'\\d+$', lambda match: ' ' * len(match.group()), s)\n",
    "    return s\n",
    "\n",
    "def read_cobol_list(file_path):\n",
    "    data = []\n",
    "    try:\n",
    "        with open(file_path, 'r', errors='ignore') as file:\n",
    "            for line in file:\n",
    "                data.append(line)\n",
    "    except UnicodeDecodeError:\n",
    "        with open(file_path, 'r', encoding='latin-1') as file:\n",
    "            for line in file:\n",
    "                data.append(line)\n",
    "    if all(len(s) == len(data[1]) for s in data[1:-1]) or all_start_with_detected_length_number(data[1:-1]):\n",
    "        data = [replace_numeric_edges_with_space(item) for item in data]\n",
    "    data = [item for item in data if not item.strip('\\n').strip()=='']\n",
    "    return data\n",
    "\n",
    "def read_cobol_no_comment_lst(file_path):\n",
    "    lst = read_cobol_list(file_path)\n",
    "    lst = [item for item in lst if not item.strip().startswith('*') and not item.startswith('      D ')]\n",
    "    for i, line in enumerate(lst):\n",
    "        if '*>' in line:\n",
    "            idx = line.index('*>')\n",
    "            lst[i] = line[:idx]+'\\n'\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_program(program_id, all_lines, programs_dict):\n",
    "    if not 'end' in programs_dict[program_id]:\n",
    "        print(f'ERROR! Program {program_id} is not ended')\n",
    "        return None\n",
    "    start, end = programs_dict[program_id]['start'],programs_dict[program_id]['end']\n",
    "    nested = []\n",
    "    lines = [i for i in all_lines]\n",
    "    for pid in programs_dict:\n",
    "        if programs_dict[pid]['start'] > start and programs_dict[pid]['end'] < end:\n",
    "            nested.append(pid)\n",
    "            if 'identification division' in lines[programs_dict[pid]['start']-1].lower():\n",
    "                lines[programs_dict[pid]['start']-1:programs_dict[pid]['end']+1] = ' '*(programs_dict[pid]['end']-programs_dict[pid]['start']+2)\n",
    "            else:\n",
    "                lines[programs_dict[pid]['start']:programs_dict[pid]['end']+1] = ' '*(programs_dict[pid]['end']-programs_dict[pid]['start']+1)\n",
    "    extracted = lines[start-1:end+1] if 'identification division' in lines[start-1].lower() else lines[start:end+1]\n",
    "    extracted = [item for item in extracted if not item == ' ']\n",
    "    return extracted  \n",
    "\n",
    "def get_programs(lines):\n",
    "    programs = {}\n",
    "    for i,line in enumerate(lines):\n",
    "        if 'program-id.' in line.lower().strip()[:11]:\n",
    "            program_id = line.strip().split()[-1].strip('.').strip(\"'\")\n",
    "            programs[program_id] = {'start':i}\n",
    "        elif 'end program' in line.lower().strip()[:11]:\n",
    "            program_id = line.strip().split()[-1].strip('.').strip(\"'\")\n",
    "            if program_id in programs:\n",
    "                programs[program_id]['end'] = i\n",
    "            else:\n",
    "                print(f'ERROR! program {program_id} start not found')\n",
    "        elif 'function-id.' in line.lower().strip()[:12]:\n",
    "            program_id = line.strip().split()[-1].strip('.').strip(\"'\")\n",
    "            programs[program_id] = {'start':i}\n",
    "        elif 'end function' in line.lower().strip()[:12]:\n",
    "            program_id = line.strip().split()[-1].strip('.').strip(\"'\")\n",
    "            if program_id in programs:\n",
    "                programs[program_id]['end'] = i\n",
    "            else:\n",
    "                print(f'ERROR! function {program_id} start not found')\n",
    "    if len(programs) <=1:\n",
    "        return None\n",
    "    # elif len(programs) == 1 and not 'end' in programs[list(programs.keys())[0]]:\n",
    "    #     return None\n",
    "    else:\n",
    "        return [extract_program(idx, lines, programs) for idx in programs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the indentation issue of this repo, otherwise read cobol functions cannot work\n",
    "for i, path in enumerate(os.listdir('../data/projects/brazilofmux_gnucobol')):\n",
    "    with open(os.path.join('../data/projects/brazilofmux_gnucobol', path), \"r\") as file:\n",
    "        lines = file.readlines()  # Reads all lines into a list\n",
    "    lines = list(map(lambda x: ' '*7 + x, lines))\n",
    "    with open(os.path.join('../data/projects/brazilofmux_gnucobol', path), 'w') as newfile:\n",
    "        newfile.writelines(lines)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_program_file = []\n",
    "seperated = []\n",
    "for file_path in file_paths:\n",
    "    file_lst = read_cobol_no_comment_lst(file_path)\n",
    "    programs = get_programs(file_lst)\n",
    "    new_folder_dir = os.path.relpath(os.path.dirname(file_path.replace('projects','project_clean')))\n",
    "    new_file_dir = os.path.join(new_folder_dir, file_path.split('/')[-1])\n",
    "    \n",
    "    # if more than 1 program in the file\n",
    "    if programs:\n",
    "        print(f\"{file_path} has {len(programs)} programs \")\n",
    "        multi_program_file.append(file_path)\n",
    "        for i, program in enumerate(programs):\n",
    "            program_dir = new_file_dir.replace('.cbl',f'_program_{i}.cbl')\n",
    "            if not os.path.exists(new_folder_dir):\n",
    "                os.makedirs(new_folder_dir)\n",
    "            with open(program_dir, 'w') as file:\n",
    "                file.writelines(program)\n",
    "            seperated.append(program_dir)\n",
    "    else:\n",
    "        if not os.path.exists(new_folder_dir):\n",
    "            os.makedirs(new_folder_dir)\n",
    "        with open(new_file_dir, 'w') as file:\n",
    "            file.writelines(file_lst)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get lists of files with and without reference data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the cobol files without reference data (i.e., with no proper head comments)\n",
    "cbl_files_not_in_dataset = []\n",
    "# the cobol files with reference data (i.e., with no proper head comments)\n",
    "cbl_files_in_dataset = file_reference_df['file path'].apply(lambda x:x.replace('/projects/','/project_clean/')).to_list()\n",
    "result = subprocess.run([\"find\", repo_clean_dir, \"-type\", \"f\", \"-name\", \"*.cbl\"], capture_output=True, text=True, check=True)\n",
    "cbl_files_not_in_dataset = result.stdout.splitlines()\n",
    "cbl_files_not_in_dataset = list(map(lambda x: os.path.relpath(x),cbl_files_not_in_dataset))\n",
    "cbl_files_not_in_dataset = [item for item in cbl_files_not_in_dataset if not item in cbl_files_in_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get copybooks for each file and insert back to data division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cobol_no_comment_str(file_path):\n",
    "    lst = read_cobol_list(file_path)\n",
    "    lst = [item for item in lst if not item.strip().startswith('*') and not item.startswith('      D ')]\n",
    "    for i, line in enumerate(lst):\n",
    "        if '*>' in line:\n",
    "            idx = line.index('*>')\n",
    "            lst[i] = line[:idx]+'\\n'\n",
    "    return ''.join(lst)\n",
    "\n",
    "def read_cobol_no_comment_lst(file_path):\n",
    "    lst = read_cobol_list(file_path)\n",
    "    lst = [item for item in lst if not item.strip().startswith('*') and not item.startswith('      D ')]\n",
    "    for i, line in enumerate(lst):\n",
    "        if '*>' in line:\n",
    "            idx = line.index('*>')\n",
    "            lst[i] = line[:idx]+'\\n'\n",
    "    return lst\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_copy(filename, file_path):\n",
    "    if 'processed_repos' in file_path:\n",
    "        path = file_path.replace('processed_repos', 'cobol_repos')\n",
    "    else:\n",
    "        path = file_path\n",
    "    if 'cics-banking-sample-application-cbsa' in path:\n",
    "        repo_path = '../data/projects'\n",
    "        copybook_path = os.path.join(repo_path,'cics-banking-sample-application-cbsa/src/base/cobol_copy')\n",
    "        code_path = os.path.join(repo_path,'cics-banking-sample-application-cbsa/src/base/cobol_src')\n",
    "        # bms_path = os.path.join(repo_path,'cics-banking-sample-application-cbsa/src/base/bms_src')\n",
    "    else:\n",
    "        copybook_path = path.replace('/'+path.split('/')[-1],'')\n",
    "        code_path = path.replace('/'+path.split('/')[-1],'')\n",
    "        # bms_path = path.replace('/'+path.split('/')[-1],'')\n",
    "    \n",
    "    copybook = None\n",
    "    if filename.strip().endswith('.cpy'):\n",
    "        if os.path.exists(os.path.join(copybook_path,filename)):\n",
    "            #print('copybook')\n",
    "            copybook = read_cobol_no_comment_str(os.path.join(copybook_path,filename))\n",
    "        # else:\n",
    "        #     print(f'Cannot find copybook {os.path.join(copybook_path,filename)}')\n",
    "    elif filename.strip().endswith('.cbl'):\n",
    "        if os.path.exists(os.path.join(code_path,filename)):\n",
    "            print('cbl')\n",
    "            copybook = read_cobol_no_comment_str(os.path.join(code_path,filename))\n",
    "        # else:\n",
    "        #     print(f'Cannot find copybook {os.path.join(code_path,filename)}')\n",
    "    elif os.path.exists(os.path.join(copybook_path,filename+'.cpy')):\n",
    "        #print('copybook')\n",
    "        copybook = read_cobol_no_comment_str(os.path.join(copybook_path,filename+'.cpy'))\n",
    "    elif os.path.exists(os.path.join(code_path,filename+'.cbl')):\n",
    "        # TODO: get data division here\n",
    "        print('cbl')\n",
    "        copybook = read_cobol_no_comment_str(os.path.join(code_path,filename+'.cbl'))\n",
    "    # elif os.path.exists(os.path.join(bms_path,filename+'.bms')):\n",
    "    #     #print('bms')\n",
    "    #     copybook = read_cobol_no_comment_str(os.path.join(bms_path,filename+'.bms'))\n",
    "    # else:\n",
    "    #     print(f'Cannot find copybook <{filename}> for {path}')\n",
    "    return copybook\n",
    "\n",
    "def is_contain_copy(copybook):\n",
    "    copybook_lst = copybook.split('\\n')\n",
    "    for line in copybook_lst:\n",
    "        if line.strip().lower().startswith('copy'):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code find the copybook referenced in the data dividion of a cobol file, \n",
    "# and insert the data in the copybook back to the data division. Now, the data division \n",
    "# in the cobol file contains the complete variable information.\n",
    "#\n",
    "# There will be a few copybooks not found from the repository.\n",
    "# Therefore, we cannot find the data included in those copybooks, and we have to neglect those data.\n",
    "# not_found: the names of the copybooks not found\n",
    "# not_found_original: the names of the files containing copybooks not found\n",
    "# new_dataset: the content of files with the copybook info inserted\n",
    "not_found, not_found_original = [], []\n",
    "new_dataset = []\n",
    "for i, file_path in enumerate(file_paths):\n",
    "    #path = os.path.join(repo_clean_dir, file_path)\n",
    "    file = read_cobol_no_comment_lst(file_path)\n",
    "    for idx, line in enumerate(file):\n",
    "        if line.strip().lower().startswith('copy'):\n",
    "            #print(line)\n",
    "            filename = line.strip().split()[-1]\n",
    "            filename = filename.strip().strip('.').strip('\"').strip(\"'\").strip('.')\n",
    "            #print(filename)\n",
    "            if 'replacing' in line.lower():\n",
    "                filename = line.strip().split()[1]\n",
    "                filename = filename.strip().strip('.').strip('\"').strip(\"'\").strip('.')\n",
    "            if '/' in filename:\n",
    "                filename = filename.split('/')[-1]\n",
    "            copy_content = get_copy(filename, file_path)\n",
    "            if not copy_content:\n",
    "                copy_content = get_copy(filename.lower(), file_path)\n",
    "            if copy_content:\n",
    "                if 'replacing' in line.lower():\n",
    "                    try:\n",
    "                        words = line.lower().strip().split()\n",
    "                        to_be_replace = line.strip().split()[words.index('replacing')+1].strip('.').strip('\"').strip(\"'\")\n",
    "                        replace_with = line.strip().split()[words.index('by')+1].strip('.').strip('\"').strip(\"'\")\n",
    "                        copy_content = copy_content.replace(to_be_replace, replace_with )\n",
    "                    except:\n",
    "                        # only one exception found here, so i hard code it\n",
    "                        copy_content = copy_content.replace('NUMBER-OF-ACCOUNTS.', 'NUMBER-OF-ACCOUNTS')\n",
    "                        if not 'COPY INQACCCU REPLACING ==NUMBER-OF-ACCOUNTS.==' in line:\n",
    "                            print(f'Error in replacing copybook keywords! Cannot find copybook <{filename}> for {file_path}')\n",
    "                file[idx] = copy_content +'\\n' if not copy_content.endswith('\\n') else copy_content\n",
    "            else:\n",
    "                print(f'{i} Cannot find copybook <{filename}> for {file_path}')\n",
    "                print('------------------------------------------------------------------')\n",
    "                not_found.append(filename)\n",
    "                not_found_original.append(file_path)\n",
    "                file[idx] = ''\n",
    "    new_dataset.append(''.join(file))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the majority of the following copybooks are found because their names are stored in variable or referenced to another copybook\n",
    "set(not_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to project_clean dir \n",
    "for i, file_path in enumerate(file_paths):\n",
    "    path = os.path.relpath(file_path.replace('projects','project_clean'))\n",
    "    # donot write to the file with mutiple programs into the project_clean dir, as we have prepared a version of these programs into seperated files\n",
    "    if file_path in multi_program_file:\n",
    "        continue\n",
    "    with open(path, 'w') as file:\n",
    "        file.write(''.join(new_dataset[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code check the files still containing unmatched reference to copybook\n",
    "# Output should be 3 files in ibmdbbdev_Samples repo, which we were not able to find\n",
    "def read_cobol_str(file_path):\n",
    "    data = read_cobol_list(file_path)\n",
    "    return '\\n'.join(data)\n",
    "\n",
    "for file_path in file_paths:\n",
    "    if file_path in multi_program_file:\n",
    "        continue\n",
    "    path = os.path.relpath(file_path.replace('projects','project_clean'))\n",
    "    file = read_cobol_str(path)\n",
    "    if is_contain_copy(file):\n",
    "        print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix indentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all files that are in the reference dataset\n",
    "file_reference_dataset = []\n",
    "for i, path in enumerate(cbl_files_in_dataset):\n",
    "    file_reference_dataset.append(read_cobol_list(path))\n",
    "\n",
    "# Fix the files that have indentation error\n",
    "for file_idx, file_lst in enumerate(file_reference_dataset):\n",
    "    # reset all lines where indent = 8 to indent = 7\n",
    "    # reset all function/section names to indent = 7\n",
    "    data_idx = 0\n",
    "    procedure_idx = 0\n",
    "    for i, line in enumerate(file_lst):\n",
    "        if 'data division' in line.lower():\n",
    "            data_idx = i\n",
    "        elif 'procedure division' in line.lower():\n",
    "            procedure_idx = i\n",
    "\n",
    "    for i, line in enumerate(file_lst):\n",
    "        if len(line) - len(line.lstrip()) == 8:\n",
    "            file_lst[i] = ' '*7 + line.lstrip()\n",
    "        if 'bmcsoftware_vscode-ispw' in cbl_files_in_dataset[file_idx]:\n",
    "            if len(line) - len(line.lstrip()) == 9:\n",
    "                if not line.strip().startswith('2 ') and not line.strip().startswith('0'):\n",
    "                    file_lst[i] = ' '*7 + line.lstrip()\n",
    "        if i > procedure_idx:\n",
    "            if line.strip().lower().endswith('section.') and len(line.strip().split()) == 2:\n",
    "                file_lst[i] = ' '*7 + line.lstrip()\n",
    "\n",
    "for i, path in enumerate(cbl_files_in_dataset):\n",
    "    with open(path, 'w') as newfile:\n",
    "        newfile.writelines(file_reference_dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all files that are not in the reference dataset\n",
    "not_file_reference_dataset = []\n",
    "for i, path in enumerate(cbl_files_not_in_dataset):\n",
    "    not_file_reference_dataset.append(read_cobol_list(path))\n",
    "\n",
    "# Fix the files that have indentation error\n",
    "for file_idx, file_lst in enumerate(not_file_reference_dataset):\n",
    "    # reset all lines where indent = 8 to indent = 7\n",
    "    # reset all function/section names to indent = 7\n",
    "    data_idx = 0\n",
    "    procedure_idx = 0\n",
    "    for i, line in enumerate(file_lst):\n",
    "        if 'data division' in line.lower():\n",
    "            data_idx = i\n",
    "        elif 'procedure division' in line.lower():\n",
    "            procedure_idx = i\n",
    "\n",
    "    for i, line in enumerate(file_lst):\n",
    "        if len(line) - len(line.lstrip()) == 8:\n",
    "            file_lst[i] = ' '*7 + line.lstrip()\n",
    "        if 'bmcsoftware_vscode-ispw' in cbl_files_not_in_dataset[file_idx]:\n",
    "            if len(line) - len(line.lstrip()) == 9:\n",
    "                if not line.strip().startswith('2 ') and not line.strip().startswith('0'):\n",
    "                    file_lst[i] = ' '*7 + line.lstrip()\n",
    "        if i > procedure_idx:\n",
    "            if line.strip().lower().endswith('section.') and len(line.strip().split()) == 2:\n",
    "                file_lst[i] = ' '*7 + line.lstrip()\n",
    "\n",
    "for i, path in enumerate(cbl_files_not_in_dataset):\n",
    "    with open(path, 'w') as newfile:\n",
    "        newfile.writelines(not_file_reference_dataset[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract function-level artifact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_division(file_lst):\n",
    "    data_idx = 0\n",
    "    procedure_idx = 0\n",
    "    for i, line in enumerate(file_lst):\n",
    "        if 'data division' in line.lower():\n",
    "            data_idx = i\n",
    "        elif 'procedure division' in line.lower():\n",
    "            procedure_idx = i\n",
    "    return file_lst[data_idx+1:procedure_idx]\n",
    "\n",
    "## extract data division info for variables used a piece of code snippet\n",
    "def has_parent(parent_dict, num_indent):\n",
    "    for item in parent_dict:\n",
    "        if item < num_indent:\n",
    "            return True\n",
    "        \n",
    "def get_parents(parent_dict, num_indent):\n",
    "    lst = []\n",
    "    for item in parent_dict:\n",
    "        if item < num_indent:\n",
    "            lst.append(parent_dict[item])\n",
    "    return lst\n",
    "\n",
    "def get_variable_in_code(data_div, code):\n",
    "    variables = data_div[:]\n",
    "    variables = [item for item in variables if not item.strip()=='']\n",
    "    line_nums = []\n",
    "    entry_end, isincode = True, False\n",
    "    parents = {} #<num ident>:<line number>\n",
    "    for i, line in enumerate(variables):\n",
    "        if entry_end: # if previous entry ended  \n",
    "            if line.strip()[0].isdigit() and len(line.strip().split()) > 1:\n",
    "                num_indent = len(line) - len(line.lstrip())\n",
    "                if num_indent <= 8:\n",
    "                    parents = {}\n",
    "                parents[num_indent] = i\n",
    "                #print(line.strip().strip('.').split())\n",
    "                # print(entry_end)\n",
    "                # if i >0:\n",
    "                #     print(variables[i-1])\n",
    "                # print(line)\n",
    "                var = line.strip().strip('.').split()[1]\n",
    "                if var.lower() in code.lower():\n",
    "                    if has_parent(parents, num_indent):\n",
    "                        parent_nodes = get_parents(parents, num_indent)\n",
    "                        for parent_node in parent_nodes:\n",
    "                            parent_line_idx = parent_node\n",
    "                            if variables[parent_line_idx].strip().endswith('.'):\n",
    "                                line_nums.append(parent_line_idx)\n",
    "                            else:\n",
    "                                parent_entry_count = 0\n",
    "                                while(True):\n",
    "                                    line_nums.append(parent_line_idx+parent_entry_count)\n",
    "                                    if variables[parent_line_idx+parent_entry_count].strip().endswith('.'):\n",
    "                                        break\n",
    "                                    parent_entry_count += 1     \n",
    "                    line_nums.append(i)\n",
    "                    isincode = True\n",
    "                    if line.strip().endswith('.'): #if this entry ends in current line\n",
    "                        isincode = False \n",
    "                        entry_end = True\n",
    "                    else:\n",
    "                        entry_end = False\n",
    "        else:\n",
    "            if isincode:\n",
    "                line_nums.append(i)\n",
    "                if line.strip().endswith('.'):\n",
    "                    entry_end = True\n",
    "                    isincode = False\n",
    "\n",
    "    line_nums = list(sorted(set(line_nums)))\n",
    "    variable_info = '\\n'.join([variables[item] for item in line_nums])\n",
    "    return variable_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## file used in these functions must have comments removed\n",
    "def get_called_paragraphs(code):\n",
    "    code_lst = code.split('\\n')\n",
    "    call_lines = code.split('\\n')\n",
    "    #call_lines = [item.strip().strip('.') for item in code_lst if item.strip().lower().startswith('perform') or item.strip().lower().startswith('go to')]\n",
    "    called_paragraphs = []\n",
    "    multiple_goto = False\n",
    "    for i, line in enumerate(call_lines):\n",
    "        if line.strip().lower().startswith('perform'):\n",
    "            words = line.split()\n",
    "            if len(words) == 2:\n",
    "                called_paragraphs.append(words[-1].strip().strip('.'))\n",
    "            elif 'thru' in line.lower():\n",
    "                lower_words = line.lower().split()\n",
    "                called_paragraphs.append(words[lower_words.index('perform')+1].strip().strip('.'))\n",
    "                called_paragraphs.append(words[lower_words.index('thru')+1].strip().strip('.'))\n",
    "            elif 'through' in line.lower():\n",
    "                lower_words = line.lower().split()\n",
    "                called_paragraphs.append(words[lower_words.index('perform')+1].strip().strip('.'))\n",
    "                called_paragraphs.append(words[lower_words.index('through')+1].strip().strip('.'))\n",
    "        elif line.strip().lower().startswith('go to'):\n",
    "            words = line.split()\n",
    "            if len(words) == 3:\n",
    "                called_paragraphs.append(words[-1].strip().strip('.'))\n",
    "            elif line.strip().lower() == 'go to':\n",
    "                multiple_goto = True\n",
    "        elif multiple_goto:\n",
    "            if 'depending on' in line.lower().strip():\n",
    "                if line.strip().lower().startswith('depending'):\n",
    "                    multiple_goto = False\n",
    "            elif len(line.strip().split())==1:\n",
    "                called_paragraphs.append(line.strip().strip('.'))\n",
    "                # print('------------------')\n",
    "                # print(call_lines[i-1])\n",
    "                # print(line)\n",
    "                # print(call_lines[i+1])\n",
    "                # print(code)\n",
    "\n",
    "    #call_lines = [item.split()[-1] for item in call_lines if len(item.split())==2]\n",
    "    return list(set(called_paragraphs))\n",
    "\n",
    "def get_section_code_with_name(pname, file):\n",
    "    # file: a string of lines in a file with with comments removed\n",
    "    file_lst = file.split('\\n')\n",
    "    line_num = [item for item in file_lst if item.strip().lower() == pname.lower()+' section.']\n",
    "    if len(line_num) <= 0:\n",
    "        return None\n",
    "    line_num = file_lst.index(line_num[0])\n",
    "    section_lst = [file_lst[line_num]]\n",
    "    for line in file_lst[line_num+1:]:\n",
    "        if line.strip().lower().endswith('section.'):\n",
    "            break\n",
    "        section_lst.append(line)\n",
    "    return '\\n'.join(section_lst)\n",
    "\n",
    "\n",
    "def get_paragraph_code_with_name(pname, file):\n",
    "    # file: a string of lines in a file with with comments removed\n",
    "    file_lst = file.split('\\n')\n",
    "    line_num = [item for item in file_lst if item.strip().lower() == pname.lower()+'.']\n",
    "    if len(line_num) <= 0:\n",
    "        return None\n",
    "    num_indent = len(line_num[0]) - len(line_num[0].lstrip())\n",
    "    line_num = file_lst.index(line_num[0])\n",
    "    paragraph_lst = [file_lst[line_num]]\n",
    "    for line in file_lst[line_num+1:]:\n",
    "        if len(line) - len(line.lstrip()) <= num_indent:\n",
    "            break\n",
    "        paragraph_lst.append(line)\n",
    "    return '\\n'.join(paragraph_lst)\n",
    "\n",
    "def get_call_hierarchy(code, file):\n",
    "    # file: a string of lines in a file with with comments removed\n",
    "    hierarchy = {}\n",
    "    called_names = get_called_paragraphs(code)\n",
    "    for pname in called_names:\n",
    "        section = get_section_code_with_name(pname, file)\n",
    "        paragraph = get_paragraph_code_with_name(pname, file)\n",
    "        if section and paragraph:\n",
    "            print(f'ERROR! {pname} is both a section and a paragraph')\n",
    "            hierarchy[pname]  ='NOT FOUND'\n",
    "        elif section or paragraph:\n",
    "            new_code = section if section else paragraph\n",
    "            sub_called = get_called_paragraphs(new_code)\n",
    "            if len(sub_called) <= 0:\n",
    "                hierarchy[pname] = None\n",
    "            else:\n",
    "                hierarchy[pname] = get_call_hierarchy(new_code, file)\n",
    "        else:\n",
    "            print(f'ERROR! {pname} is not found in file')\n",
    "            hierarchy[pname]  ='NOT FOUND'\n",
    "    return hierarchy\n",
    "        \n",
    "def print_call_hierarchy(d, indent=0):\n",
    "    for key, value in d.items():\n",
    "        print('    ' * indent + str(key))\n",
    "        if isinstance(value, dict):\n",
    "            print_call_hierarchy(value, indent + 1)\n",
    "        else:\n",
    "            print('    ' * (indent + 1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the code for all functions (i.e., section or paragraph in a procedure division)\n",
    "## return: section_dict    -> {<section name>: <section code>, ...}\n",
    "##         paragraph_dict  -> {<paragraph name>: {'section': <section name>/None, \n",
    "##                                                'code': <paragraph code>}, \n",
    "##                             ...}\n",
    "def get_functions(program_lst):\n",
    "    procedure_start = [i for i,line in enumerate(program_lst) if 'procedure division' in line.lower()]\n",
    "    if len(procedure_start) > 1:\n",
    "        print('ERROR! More than 1 procedure division found in program.')\n",
    "        return None\n",
    "    elif len(procedure_start) <= 0:\n",
    "        print('ERROR! No procedure division found in program.')\n",
    "        return None\n",
    "\n",
    "    procedure_start = procedure_start[0]\n",
    "    standard_indent =  len(program_lst[procedure_start])-len(program_lst[procedure_start].lstrip())\n",
    "    sections, paragraphs = [], []\n",
    "    current_section, current_paragraph = {}, {}\n",
    "    start_using = False\n",
    "    for i, line in enumerate(program_lst[procedure_start+1:]):\n",
    "        if len(line)-len(line.lstrip()) < standard_indent:\n",
    "            print(f'ERROR! Anomaly indentation detected in line {i+procedure_start+1} in program:\\n{line}')\n",
    "        else:\n",
    "            if len(line)-len(line.lstrip()) == standard_indent:\n",
    "                if line.strip().lower().endswith('section.') and len(line.strip().split())==2:\n",
    "                    if 'start' in current_section:\n",
    "                        current_section['end'] = i-1\n",
    "                        sections.append(current_section)\n",
    "                        current_section = {}\n",
    "                    current_section['name'] = line.strip().split()[0].strip('.').strip(\"'\").strip('\"')\n",
    "                    current_section['start'] = i\n",
    "                    if 'start' in current_paragraph:\n",
    "                        current_paragraph['end'] = i-1\n",
    "                        paragraphs.append(current_paragraph)\n",
    "                        current_paragraph = {}\n",
    "                    \n",
    "                elif line.strip().endswith('.') and len(line.strip().split())==1:\n",
    "                    if 'start' in current_paragraph:\n",
    "                        current_paragraph['end'] = i-1\n",
    "                        paragraphs.append(current_paragraph)\n",
    "                        current_paragraph = {}\n",
    "                    current_paragraph['name'] = line.strip().strip('.').strip(\"'\").strip('\"')\n",
    "                    current_paragraph['start'] = i\n",
    "                    \n",
    "                elif line.lower().strip().startswith('end program') or line.lower().strip().startswith('end function'):\n",
    "                    i -=1\n",
    "                    break\n",
    "                else:\n",
    "                    print(f'Line {i+procedure_start+1} in program is not a section or a program:\\n{line}')\n",
    "\n",
    "            if len(sections) == 0 and len(paragraphs) == 0 and not 'start' in current_paragraph and not 'start' in current_section:\n",
    "                if i == 0 and not 'using' in program_lst[procedure_start].lower() and not line.strip().lower().startswith('using'): # if next line of procedure division, but have bigger indentation, still consider this as a paragraph\n",
    "                    current_paragraph = {}\n",
    "                    current_paragraph['name'] = 'Top function with no name'\n",
    "                    current_paragraph['start'] = i\n",
    "                elif i == 0 and 'using' in program_lst[procedure_start].lower() and program_lst[procedure_start].strip().endswith('.'):\n",
    "                    current_paragraph = {}\n",
    "                    current_paragraph['name'] = 'Top function with no name'\n",
    "                    current_paragraph['start'] = i\n",
    "                elif i==0 and ('using' in line.lower() or 'using' in program_lst[procedure_start].lower()) and line.strip().endswith('.'):\n",
    "                    start_using = False\n",
    "                elif i==0 and ('using' in line.lower() or 'using' in program_lst[procedure_start].lower()):\n",
    "                    start_using = True\n",
    "                elif start_using == True and line.strip().endswith('.'):\n",
    "                    start_using = False\n",
    "                elif start_using == False:\n",
    "                    current_paragraph = {}\n",
    "                    current_paragraph['name'] = 'Top function with no name'\n",
    "                    current_paragraph['start'] = i\n",
    "\n",
    "\n",
    "\n",
    "    if 'start' in current_section:\n",
    "        current_section['end'] = i\n",
    "        sections.append(current_section)\n",
    "    if 'start' in current_paragraph:\n",
    "        current_paragraph['end'] = i\n",
    "        paragraphs.append(current_paragraph)\n",
    "        \n",
    "    ## check if any end > start \n",
    "    if len(sections) > 0:\n",
    "        for section in sections:\n",
    "            if section['end'] <= section['start']:\n",
    "                print(f'Incorrect lines detected for section: {section[\"name\"]}')\n",
    "    for paragraph in paragraphs:\n",
    "        if paragraph['end'] <= paragraph['start']:\n",
    "            print(f'Incorrect lines detected for paragraph: {paragraph[\"name\"]}')\n",
    "\n",
    "    # final results: dict for section and paragraphs\n",
    "    section_dict, paragraph_dict = {}, {}\n",
    "    for section in sections:\n",
    "        section_dict[section['name']] = ''.join(program_lst[procedure_start+1+section['start']:procedure_start+1+section['end']+1])\n",
    "    for paragraph in paragraphs:\n",
    "        umbrella_section = list(filter(lambda x: paragraph['start']> x['start'] and paragraph['end'] <= x['end'], sections))\n",
    "        if len(umbrella_section) == 0:\n",
    "            umbrella_section = None\n",
    "        elif len(umbrella_section) == 1:\n",
    "            umbrella_section = umbrella_section[0]['name']\n",
    "        else:\n",
    "            print(f'ERROR! Multiple section for paragraph: {paragraph[\"name\"]} detected: [{\", \".join([item[\"name\"] for item in umbrella_section])}]')\n",
    "            umbrella_section = None\n",
    "        paragraph_dict[paragraph['name']] = {'section':umbrella_section, 'code':''.join(program_lst[procedure_start+1+paragraph['start']:procedure_start+1+paragraph['end']+1])}\n",
    "    section_dict.pop('',None)\n",
    "    paragraph_dict.pop('',None)\n",
    "    return section_dict, paragraph_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code, variable, function call relation extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  functions in function-level reference dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_reference_df = pd.read_csv(os.path.join('..','data','function_level_reference_dataset.csv'), index_col = 0)\n",
    "function_reference_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the result of the following code is already contained in \"data/function_level_reference_dataset.csv\"\n",
    "function_reference_df['called paragraphs'] = function_reference_df['code'].apply(get_called_paragraphs)\n",
    "function_reference_df['variables'] = function_reference_df.apply(lambda x: get_variable_in_code(get_data_division(read_cobol_list(os.path.relpath(x['file path'].replace('projects','project_clean')))), x['code']), axis = 1)\n",
    "function_reference_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions within the files in the file-level reference dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get sections and paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is fine if there are a few errors here, as long as the errors are not on the line of paragraph or function name. \n",
    "# These errors are already internally handled.\n",
    "# merged dict contains all the section names and paragraph names, even if the paragraph is under a section. This is because the code can directly call a paragraph by its name, even if it is under a section.\n",
    "merged = []\n",
    "paragraphs, sections = [], []\n",
    "for i, path in enumerate(cbl_files_in_dataset):\n",
    "    section_dict, paragraph_dict = get_functions(read_cobol_list(os.path.relpath(path.replace('projects','project_clean'))))\n",
    "    for key in section_dict:\n",
    "        merged.append({'file path':path, 'function name':key,'code':section_dict[key], 'is section': True, 'is paragraph': False, 'section name': key })\n",
    "    for key in paragraph_dict:\n",
    "        merged.append({'file path':path, 'function name':key,'code':paragraph_dict[key]['code'], 'is section': False, 'is paragraph': True, 'section name': paragraph_dict[key]['section'] })\n",
    "    paragraphs.append(paragraph_dict)\n",
    "    sections.append(section_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for files with no section and no paragraph\n",
    "# The correct output should be empty\n",
    "for i in range(len(sections)):\n",
    "    if len(sections[i]) == 0 and len(paragraphs[i]) == 0:\n",
    "        print(cbl_files_in_dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_function_df = pd.DataFrame(merged)\n",
    "merged_function_df['file path'] = merged_function_df['file path'].apply(lambda x:x.replace(repo_clean_dir+'/',''))\n",
    "merged_function_df['repo'] = None\n",
    "merged_function_df.loc[merged_function_df['repo'].isna(), 'repo'] = merged_function_df.loc[merged_function_df['repo'].isna()]['file path'].apply(lambda x: x.split('/')[3])\n",
    "merged_function_df['called paragraphs'] = merged_function_df['code'].apply(get_called_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_called(row, df):\n",
    "    all_called = df.loc[df['file path']==row['file path']]['called paragraphs']\n",
    "    functions = []\n",
    "    for item in all_called:\n",
    "        functions.extend(item)\n",
    "    return row['function name'] in functions\n",
    "\n",
    "merged_function_df['is called'] = merged_function_df.apply(lambda x: check_called(x,merged_function_df),axis=1)\n",
    "merged_function_df['variables'] = merged_function_df.apply(lambda x: get_variable_in_code(get_data_division(read_cobol_list(os.path.relpath(path.replace('projects','project_clean')))), x['code']), axis = 1)\n",
    "merged_function_df.loc[(merged_function_df['function name']=='Top function with no name')&(merged_function_df['code'].apply(lambda x:x.strip().lower().startswith('set environment'))), 'function name'] = 'set environment'\n",
    "function_for_file_df = merged_function_df[['repo', 'file path', 'function name', 'code', 'variables', 'called paragraphs', 'is called', 'is section', 'is paragraph', 'section name']]\n",
    "function_for_file_df\n",
    "# The result of this dataframe is included in  /data/function_refined_summary.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### other functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is fine if there are a few errors here, as long as the errors are not on the line of paragraph or function name. \n",
    "# These errors are already internally handled.\n",
    "# merged dict contains all the section names and paragraph names, even if the paragraph is under a section. This is because the code can directly call a paragraph by its name, even if it is under a section.\n",
    "merged = []\n",
    "paragraphs, sections = [], []\n",
    "for i, path in enumerate(cbl_files_not_in_dataset):\n",
    "    print(os.path.relpath(path.replace('projects','project_clean')))\n",
    "    code_snippet = read_cobol_list(os.path.relpath(path.replace('projects','project_clean')))\n",
    "    section_dict, paragraph_dict = get_functions(code_snippet)\n",
    "    for key in section_dict:\n",
    "        merged.append({'file path':path, 'function name':key,'code':section_dict[key], 'is section': True, 'is paragraph': False, 'section name': key })\n",
    "    for key in paragraph_dict:\n",
    "        merged.append({'file path':path, 'function name':key,'code':paragraph_dict[key]['code'], 'is section': False, 'is paragraph': True, 'section name': paragraph_dict[key]['section'] })\n",
    "    paragraphs.append(paragraph_dict)\n",
    "    sections.append(section_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_other_function_df = pd.DataFrame(merged)\n",
    "merged_other_function_df['file path'] = merged_other_function_df['file path'].apply(lambda x:x.replace(repo_clean_dir+'/',''))\n",
    "merged_other_function_df['repo'] = None\n",
    "merged_other_function_df.loc[merged_other_function_df['repo'].isna(), 'repo'] = merged_other_function_df.loc[merged_other_function_df['repo'].isna()]['file path'].apply(lambda x: x.split('/')[3])\n",
    "merged_other_function_df['called paragraphs'] = merged_other_function_df['code'].apply(get_called_paragraphs)\n",
    "merged_other_function_df['is called'] = merged_other_function_df.apply(lambda x: check_called(x,merged_other_function_df),axis=1)\n",
    "merged_other_function_df['variables'] = merged_other_function_df.apply(lambda x: get_variable_in_code(get_data_division(read_cobol_list(os.path.relpath(path.replace('projects','project_clean')))), x['code']), axis = 1)\n",
    "merged_other_function_df.loc[(merged_other_function_df['function name']=='Top function with no name')&(merged_other_function_df['code'].apply(lambda x:x.strip().lower().startswith('set environment'))), 'function name'] = 'set environment'\n",
    "other_function_df = merged_other_function_df[['repo', 'file path', 'function name', 'code', 'variables', 'called paragraphs', 'is called', 'is section', 'is paragraph', 'section name']]\n",
    "other_function_df\n",
    "# The result of this dataframe is included in  /data/function_refined_summary.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge all functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The result of this dataframe is included in  /data/function_refined_summary.csv\n",
    "all_functions_df = pd.concat([function_for_file_df, other_function_df], ignore_index=True)\n",
    "all_functions_df['file path'] = all_functions_df['file path'].apply(lambda x:x.replace('project_clean', 'project'))\n",
    "all_functions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract calls to exernal files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_link(content):\n",
    "    link_programs = set()  # Using a set to avoid duplicates\n",
    "    call_programs = set()  # Using a set to avoid duplicates\n",
    "    file_calls = set()     # Using a set to avoid duplicates\n",
    "    maps_used = set()      # Using a set to avoid duplicates\n",
    "    trans_ids = set()      # Using a set to avoid duplicates\n",
    "    sql_tables = set()     # Using a set to avoid duplicates\n",
    "    lines = content.splitlines()\n",
    "    print(lines)\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i].strip()\n",
    "        line_upper = line.upper()\n",
    "        link_pattern = re.compile(r\"\\(([^)]+)\\)\")\n",
    "        if \"EXEC CICS LINK PROGRAM\" in line_upper:\n",
    "            # We check both the original line and the line with comments removed\n",
    "\n",
    "            matches = link_pattern.findall(line_upper)\n",
    "            for match in matches:\n",
    "                # Keep the content inside parentheses\n",
    "                link_program = match.strip()\n",
    "                link_programs.add(link_program)\n",
    "        if \"CALL '\" in line_upper:\n",
    "            start = line_upper.find(\"CALL '\") + len(\"CALL '\")\n",
    "            end = line_upper.find(\"'\", start)\n",
    "            call_program = line[start:end]\n",
    "            call_programs.add(call_program)\n",
    "        if \"EXEC CICS READ\" in line_upper or \"EXEC CICS WRITE\" in line_upper:\n",
    "            start = line_upper.find('(') + 1\n",
    "            end = line_upper.find(')', start)\n",
    "            file_call = line[start:end]\n",
    "            file_calls.add(file_call)\n",
    "        if \"EXEC CICS SEND MAP\" in line_upper:\n",
    "            start = line_upper.find('(') + 1\n",
    "            end = line_upper.find(')', start)\n",
    "            map_used = line[start:end]\n",
    "            maps_used.add(map_used)\n",
    "        if \"EXEC CICS RETURN\" in line_upper:\n",
    "            j = i\n",
    "            while j < len(lines) - 1:\n",
    "                j += 1\n",
    "                line_upper = lines[j].upper()\n",
    "                if \"TRANSID('\" in line_upper:\n",
    "                    transid_start = line_upper.find(\"TRANSID('\") + len(\"TRANSID('\")\n",
    "                    transid_end = line_upper.find(\"')\", transid_start)\n",
    "                    trans_id = lines[j][transid_start:transid_end]\n",
    "                    trans_ids.add(trans_id)\n",
    "                    break\n",
    "        if \"EXEC SQL\" in line_upper:\n",
    "            k = i\n",
    "            while k < len(lines) - 1:\n",
    "                k += 1\n",
    "                line_upper = lines[k].upper()\n",
    "                if \"FROM\" in line_upper:\n",
    "                    start = line_upper.find(\"FROM\") + len(\"FROM\")\n",
    "                    table_names = lines[k][start:].split(',')\n",
    "                    for table_name in table_names:\n",
    "                        sql_tables.add(table_name.strip())\n",
    "                    break\n",
    "        i += 1\n",
    "\n",
    "    link_status = \"LINK\" if link_programs else \"NO LINK\"\n",
    "    call_status = \"CALL\" if call_programs else \"NO CALL\"\n",
    "    file_call_status = \"FILE CALL\" if file_calls else \"NO FILE CALL\"\n",
    "    map_status = \"MAP USED\" if maps_used else \"NO MAP USED\"\n",
    "    return_status = \"RETURN\" if trans_ids else \"NO RETURN\"\n",
    "    sql_status = \"SQL\" if sql_tables else \"NO SQL\"\n",
    "    return([\", \".join(link_programs), link_status,\n",
    "                        \", \".join(call_programs), call_status,\n",
    "                        \", \".join(file_calls), file_call_status,\n",
    "                        \", \".join(maps_used), map_status,\n",
    "                        \", \".join(trans_ids), return_status,\n",
    "                        \", \".join(sql_tables), sql_status])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The result of this dataframe is included in  /data/function_refined_summary.csv\n",
    "all_functions_df[[\"Linked Programs\", \"LINK Status\", \n",
    "                                       \"Called Programs\", \"CALL Status\", \"File Calls\", \"FILE CALL Status\",\n",
    "                                       \"Maps Used\", \"MAP Status\", \"Transaction IDs\", \"RETURN Status\",\n",
    "                                       \"SQL Tables\", \"SQL Status\"]] = all_functions_df['code'].apply(check_link).apply(pd.Series)\n",
    "all_functions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract file-level artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### procedure/data division, filename, program id, paragragh names, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paragraphs_from_function(path, all_function_df):\n",
    "    functions = all_function_df.loc[all_function_df['file path']==path]\n",
    "    return functions['function name'].to_list()\n",
    "def get_program_id(code):\n",
    "    code_lst = code.splitlines()\n",
    "    for line in code_lst:\n",
    "        if line.lower().strip().startswith('program-id'):\n",
    "            return line.strip().split()[-1].strip().strip('.').strip()\n",
    "    return None\n",
    "def get_procedure_division(file_content):\n",
    "    file_lst = file_content.splitlines()\n",
    "    procedure_idx = None\n",
    "    for i, line in enumerate(file_lst):\n",
    "        if line.lower().strip().startswith('procedure division'):\n",
    "            procedure_idx = i\n",
    "            break\n",
    "    return '\\n'.join(file_lst[procedure_idx:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_file_df = pd.DataFrame(data={\"file path\": cbl_files_in_dataset + cbl_files_not_in_dataset})\n",
    "all_file_df['repo'] = all_file_df['file path'].apply(lambda x:x.split('/')[3])\n",
    "all_file_df['filename'] = all_file_df['file path'].apply(lambda x:x.split('/')[-1])\n",
    "all_file_df['program id'] = all_file_df['file path'].apply(lambda x:get_program_id(read_cobol_str(x)))\n",
    "all_file_df['code'] = all_file_df['file path'].apply(lambda x:read_cobol_str(x))\n",
    "all_file_df['procedure division'] = all_file_df['file path'].apply(lambda x:get_procedure_division(read_cobol_str(x)))\n",
    "all_file_df['data division'] = all_file_df['file path'].apply(lambda x:''.join(get_data_division(read_cobol_list(x))))\n",
    "all_file_df['file path'] = all_file_df['file path'].apply(lambda x:x.replace('project_clean', 'project'))\n",
    "all_file_df['paragraphs'] = all_file_df['file path'].apply(lambda x: get_paragraphs_from_function(x, all_functions_df))\n",
    "all_file_df\n",
    "# The result of this dataframe is included in  /data/file_generated_summary.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### paragraph call relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_call_relation_from_paragraph_df(row, all_function_df):\n",
    "    relation = ''\n",
    "    for p in row['paragraphs']:\n",
    "        temp = all_function_df.loc[(all_function_df['file path']==row['file path'])&(all_function_df['function name']==p)]#['called paragraphs']\n",
    "        if len(temp) >= 1:\n",
    "            if temp['is paragraph'].values[0] and isinstance(temp['section name'].values[0], str):\n",
    "                continue\n",
    "            else:\n",
    "                temp = temp['called paragraphs'].values[0]\n",
    "                if len(temp) >=1 :\n",
    "                    relation += f'Paragraph {p} calls paragraph '+', '.join(temp) + '.\\n'\n",
    "    return relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_file_df['call relations'] = all_file_df.apply(lambda x: get_call_relation_from_paragraph_df(x, all_functions_df),axis=1)\n",
    "all_file_df\n",
    "# The result of this dataframe is included in  /data/file_generated_summary.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### file call relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_file_df[[\"Linked Programs\", \"LINK Status\", \n",
    "                                       \"Called Programs\", \"CALL Status\", \"File Calls\", \"FILE CALL Status\",\n",
    "                                       \"Maps Used\", \"MAP Status\", \"Transaction IDs\", \"RETURN Status\",\n",
    "                                       \"SQL Tables\", \"SQL Status\"]] = all_file_df['code'].apply(check_link).apply(pd.Series)\n",
    "all_file_df\n",
    "# The result of this dataframe is included in  /data/file_generated_summary.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract project-level artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
