{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.translate.chrf_score import sentence_chrf\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from scipy.stats import mannwhitneyu\n",
    "from cliffs_delta import cliffs_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.relpath(os.path.join(os.getcwd(), '..', 'data'))\n",
    "all_paragraphs = pd.read_csv(os.path.join('..', 'data', 'function_refined_summary.csv'), index_col=0)\n",
    "all_paragraphs['called paragraphs'] = all_paragraphs['called paragraphs'].apply(lambda x:x.strip(']').strip('[').strip('\"').strip('\"').replace(\"'\",\"\").split(', '))\n",
    "index = all_paragraphs.loc[(all_paragraphs['called paragraphs'].apply(len)==1)&(all_paragraphs['called paragraphs'].apply(lambda x:x[0])=='')].index\n",
    "all_paragraphs.loc[index,'called paragraphs'] = all_paragraphs.loc[index]['called paragraphs'].apply(lambda x: [])\n",
    "all_files = pd.read_csv(os.path.join('..', 'data', 'file_generated_summary.csv'), index_col=0)\n",
    "function_reference = pd.read_csv(os.path.join('..', 'data', 'function_level_reference_dataset.csv'), index_col=0)\n",
    "file_reference = pd.read_csv(os.path.join('..', 'data', 'file_level_reference_dataset.csv'), index_col=0)\n",
    "project_df = pd.read_csv(os.path.join('..', 'data', 'project_generated_summary.csv'), index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup text similarity evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meteor(candidate_summary, reference_summary, rslt_str=''):\n",
    "    candidate_summaries_tokenized = [summary.split() for summary in candidate_summary]\n",
    "    reference_summaries_tokenized = [summary.split() for summary in reference_summary]\n",
    "    meteor_scores = []\n",
    "    # Compute METEOR score\n",
    "    for i in range(len(candidate_summaries_tokenized)):\n",
    "        meteor_scores.append(meteor_score([reference_summaries_tokenized[i]], candidate_summaries_tokenized[i]))\n",
    "    print(f\"Average METEOR Score {rslt_str}:\", round(np.mean(meteor_scores),3))\n",
    "    print(f\"Median METEOR Score {rslt_str}:\", round(np.median(meteor_scores),3))\n",
    "    return meteor_scores\n",
    "\n",
    "def chrf(candidate_summary, reference_summary, rslt_str=''):\n",
    "    # Compute sentence-level chrF scores\n",
    "    chrf_scores = [sentence_chrf(ref, hyp) for ref, hyp in zip(reference_summary, candidate_summary)]\n",
    "\n",
    "    # Compute the average (corpus-level) and median chrF scores\n",
    "    avg_score = np.mean(chrf_scores)\n",
    "    median_score = np.median(chrf_scores)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Average chrF Score {rslt_str}: {round(avg_score, 3)}\")\n",
    "    print(f\"Median chrF Score {rslt_str}: {round(median_score, 3)}\")\n",
    "\n",
    "    return chrf_scores\n",
    "\n",
    "def sentencebert(candidate_summary, reference_summary, rslt_str=''):\n",
    "    # Generate embeddings\n",
    "    sentenceBert_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "    cosine_scores = []\n",
    "    for i in range(len(candidate_summary)):\n",
    "        candidate_embedding = sentenceBert_model.encode(candidate_summary[i], convert_to_tensor=True).cpu()\n",
    "        reference_embedding = sentenceBert_model.encode(reference_summary[i], convert_to_tensor=True).cpu()\n",
    "        # Calculate cosine similarity\n",
    "        cosine_scores.append(list(util.pytorch_cos_sim(candidate_embedding, reference_embedding).numpy()[0])[0])\n",
    "    print(f\"Average sentenceBERT Score {rslt_str}:\", round(np.mean(cosine_scores),4))\n",
    "    print(f\"Median sentenceBERT Score {rslt_str}:\", round(np.median(cosine_scores),4))\n",
    "    return cosine_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load granite 34b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "model_id = \"ibm-granite/granite-34b-code-instruct\" \n",
    "device_map = device_map = \"auto\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=device_map\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model.eval()\n",
    "runtimeFlag = \"cuda:1\" \n",
    "cache_dir = None \n",
    "scaling_factor = 1.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_baseline_prompt(code_snippet):\n",
    "    return f\"### Instruction:\\nSummarize the following COBOL code:\\n{code_snippet}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in function_reference.index:\n",
    "    row = function_reference.loc[idx]\n",
    "    prompt = build_baseline_prompt(row['code'])\n",
    "    chat = [{ \"role\": \"user\", \"content\":  prompt}]\n",
    "    chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "    input_tokens = tokenizer(\n",
    "        chat,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=True\n",
    "    ).input_ids.to(runtimeFlag)\n",
    "\n",
    "    max_context = int(model.config.max_position_embeddings*scaling_factor)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    # max_prompt_len = int(0.85 * max_context)\n",
    "    # max_gen_len = int(0.10 * max_prompt_len)\n",
    "    max_prompt_len = 1024\n",
    "    max_gen_len = 1024\n",
    "\n",
    "    output = model.generate(input_ids=input_tokens, max_new_tokens=150)\n",
    "    new_tokens = output[0][input_tokens.shape[-1]:]\n",
    "    response = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "    function_reference.at[idx, 'baseline'] = response\n",
    "    print(idx, 'done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code processsing agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FUNCTION_CODE_SYSTEM_PROMPT = \"\"\"Now, you are assistant to help me explain the COBOL code and answer questions about the code. I will give you the COBOL code within <Code> and variable definitions within <Variable> tags. Your answers should be concise and coherent.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_paragraph_code_prompt(code_snippet, variables):\n",
    "    return f\"<Code>\\n{code_snippet}\\n<\\Code>\\n<Variable>\\n{variables}\\n<\\Variable>\\nGenerate an explanation of the above COBOL code. The explanation should be no more than 75 words.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in all_paragraphs.index:\n",
    "    row = all_paragraphs.loc[idx]\n",
    "    prompt = build_paragraph_code_prompt(row['code'],row['variables'])\n",
    "    chat = [{ \"role\": \"system\", \"content\":  FUNCTION_CODE_SYSTEM_PROMPT},\n",
    "            { \"role\": \"user\", \"content\":  prompt}]\n",
    "    chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "    input_tokens = tokenizer(\n",
    "        chat,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=True\n",
    "    ).input_ids.to(runtimeFlag)\n",
    "\n",
    "    max_context = int(model.config.max_position_embeddings*scaling_factor)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    # max_prompt_len = int(0.85 * max_context)\n",
    "    # max_gen_len = int(0.10 * max_prompt_len)\n",
    "    max_prompt_len = 1024\n",
    "    max_gen_len = 1024\n",
    "\n",
    "    output = model.generate(input_ids=input_tokens, max_new_tokens=150)\n",
    "    new_tokens = output[0][input_tokens.shape[-1]:]\n",
    "    response = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "    all_paragraphs.at[idx, 'generated summary'] = response\n",
    "    print(idx, 'done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the explanations of the reference dataset and run text similarity evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in function_reference.index:\n",
    "    row = function_reference.loc[idx]\n",
    "    prompt = build_paragraph_code_prompt(row['code'],row['variables'])\n",
    "    chat = [{ \"role\": \"system\", \"content\":  FUNCTION_CODE_SYSTEM_PROMPT},\n",
    "            { \"role\": \"user\", \"content\":  prompt}]\n",
    "    chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "    input_tokens = tokenizer(\n",
    "        chat,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=True\n",
    "    ).input_ids.to(runtimeFlag)\n",
    "\n",
    "    max_context = int(model.config.max_position_embeddings*scaling_factor)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    # max_prompt_len = int(0.85 * max_context)\n",
    "    # max_gen_len = int(0.10 * max_prompt_len)\n",
    "    max_prompt_len = 1024\n",
    "    max_gen_len = 1024\n",
    "\n",
    "    output = model.generate(input_ids=input_tokens, max_new_tokens=150)\n",
    "    new_tokens = output[0][input_tokens.shape[-1]:]\n",
    "    response = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "    function_reference.at[idx, 'refined summary2'] = response\n",
    "    print(idx, 'done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rslt1 = meteor(function_reference['baseline'], function_reference['reference data'], rslt_str='for baseline')\n",
    "rslt2 = meteor(function_reference['refined summary2'], function_reference['reference data'], rslt_str='for generated')\n",
    "print(f'METEOR pvalue {mannwhitneyu(rslt1, rslt2).pvalue}')\n",
    "print(f'METEOR effect size {cliffs_delta(rslt1, rslt2)[1]}')\n",
    "rslt1 = chrf(function_reference['baseline'], function_reference['reference data'], rslt_str='for baseline')\n",
    "rslt2 = chrf(function_reference['refined summary2'], function_reference['reference data'], rslt_str='for generated')\n",
    "print(f'chrF pvalue {mannwhitneyu(rslt1, rslt2).pvalue}')\n",
    "print(f'chrF effect size {cliffs_delta(rslt1, rslt2)[1]}')\n",
    "rslt1 = sentencebert(function_reference['baseline'].to_list(), function_reference['reference data'].to_list(), rslt_str='for baseline')\n",
    "rslt2 = sentencebert(function_reference['refined summary2'].to_list(), function_reference['reference data'].to_list(), rslt_str='for generated')\n",
    "print(f'sentenceBert pvalue {mannwhitneyu(rslt1, rslt2).pvalue}')\n",
    "print(f'sentenceBert effect size {cliffs_delta(rslt1, rslt2)[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text processing agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    # Defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key = '<your own openai key'\n",
    ")\n",
    "gpt_model_id = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_paragraphs_ready(row, all_paragraphs):\n",
    "    paragraphs = row['called paragraphs']\n",
    "    df = all_paragraphs.loc[(all_paragraphs['file path']==row['file path'])&(all_paragraphs['function name'].isin(paragraphs))]\n",
    "    if not df['refined summary2'].isna().any():\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAGRAPH_SYSTEM_PROMPT = \"\"\"You are now an writing assistant to help me simplify a paragraph containing complex terms. I will provide the main paragraph within <Main> tags and the definitions of the terms within <Term> tags. Your task is to replace the complex terms with concise descriptions to improve readability, while making as few changes as possible to the main paragraph.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_function_writing_prompt(row, paragraph_df):\n",
    "    code_snippet, called_paragraph = row['generated summary'], row['called paragraphs']\n",
    "    called_paragraph_text = ''\n",
    "    if len(called_paragraph) > 0:\n",
    "        df = paragraph_df.loc[paragraph_df['file path']==row['file path']]\n",
    "        df = df.loc[df['function name'].isin(row['called paragraphs'])]\n",
    "        called_paragraph_text = \"\"\n",
    "        for p in called_paragraph:\n",
    "            called_paragraph_text += f'{p}: {df.loc[df[\"function name\"]==p][\"refined summary2\"].values[0]}\\n'\n",
    "    return f\"\\n<Main>\\n{code_snippet}\\n</Main>\\n\\n<Term>\\n{called_paragraph_text}\\n</Term>\\nGenerate the improved main function:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_paragraphs['refined summary2'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the paragraph with no called functions, the final explanation is already generated by text processing agent, and don't need to use text processing agent\n",
    "temp_df = all_paragraphs.loc[all_paragraphs['called paragraphs'].apply(len)==0]\n",
    "all_paragraphs.loc[temp_df.index, 'refined summary2'] = all_paragraphs.loc[temp_df.index, 'generated summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unexplained_last_iter = -1 # number of paragraph not explained in last iteration, check circular dependency\n",
    "num_ready_last_iter = -1\n",
    "while True:\n",
    "    idx_paragraph_ready = all_paragraphs.loc[(all_paragraphs['refined summary2'].isna()&(all_paragraphs.apply(lambda x: check_paragraphs_ready(x, all_paragraphs), axis=1)))].index\n",
    "    print(len(idx_paragraph_ready), 'paragraphs ready to explain')\n",
    "    if len(idx_paragraph_ready) == num_ready_last_iter and len(all_paragraphs.loc[all_paragraphs['refined summary2'].isna()]) == num_unexplained_last_iter:\n",
    "        if len(idx_paragraph_ready)==0: # all explained\n",
    "            break\n",
    "        else: #circular dependency\n",
    "            all_paragraphs.at[idx_paragraph_ready[0], 'refined summary2'] = all_paragraphs.at[idx_paragraph_ready[0], 'generated summary']\n",
    "            continue\n",
    "    num_unexplained_last_iter = len(all_paragraphs.loc[all_paragraphs['refined summary2'].isna()]) \n",
    "    num_ready_last_iter = len(idx_paragraph_ready)\n",
    "    for idx in idx_paragraph_ready:\n",
    "        row = all_paragraphs.loc[idx]\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": PARAGRAPH_SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": build_function_writing_prompt(row, all_paragraphs)}\n",
    "        ]\n",
    "\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            model=gpt_model_id,\n",
    "            messages=messages\n",
    "            )\n",
    "        response = chat_completion.choices[0].message.content.replace('<Main>','').replace('</Main>','').strip()\n",
    "        all_paragraphs.at[idx, 'refined summary2'] = response\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_baseline_prompt_file(code):\n",
    "    code_lst = code.split('\\n')\n",
    "    for i, line in enumerate(code_lst):\n",
    "        if 'procedure division' in line.lower():\n",
    "            break\n",
    "    return 'Summarize the following COBOL file:\\n'+'\\n'.join(code_lst[i:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in file_reference.index:\n",
    "    try:\n",
    "        row = file_reference.loc[idx]\n",
    "        prompt = build_baseline_prompt_file(row['code'])\n",
    "        chat = [{ \"role\": \"user\", \"content\":  prompt}]\n",
    "        chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "        input_tokens = tokenizer(\n",
    "            chat,\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=True\n",
    "        ).input_ids.to(runtimeFlag)\n",
    "\n",
    "        max_context = int(model.config.max_position_embeddings*scaling_factor)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        # max_prompt_len = int(0.85 * max_context)\n",
    "        # max_gen_len = int(0.10 * max_prompt_len)\n",
    "        max_prompt_len = 1024\n",
    "        max_gen_len = 1024\n",
    "\n",
    "        output = model.generate(input_ids=input_tokens, max_new_tokens=150)\n",
    "        new_tokens = output[0][input_tokens.shape[-1]:]\n",
    "        response = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "        file_reference.at[idx, 'baseline'] = response\n",
    "        print(idx, 'done')\n",
    "    except RuntimeError as e1:\n",
    "        file_reference.at[idx, 'baseline'] = 'Error Long'\n",
    "        print(idx, 'Error long')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHORT_FILE_SYSTEM_PROMPT = 'Now, you are assistant to help me explain a COBOL file. I will give you the procedure division of the COBOL file embraced by <Code> tags. The data division of the file is given with <variable> tags. Generate a summary of the file base on the provided procedure and data divisoin. The summary should be a single cohesive paragraph with no more than 75 words that explains the basic business purpose and logic of the COBOL file. Ensure the summary is concise and cohesive.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_file_granite_prompt(row):\n",
    "    program_id = row['program id']\n",
    "    file_name = row['filename']\n",
    "    variables = row['data division']\n",
    "    code = row['procedure division']\n",
    "    basic_prompt = f\"This COBOL file's name is {file_name} and the program ID is {program_id}. Following is the procedure and data division of the file. \\n<Code>\\n{code}\\n</Code>\\n\\n<Variable>\\n{variables}\\n</Variable>\\nGenerate a very short and cohesive explanation of the COBOL file. The explanation should be no more than 75 words.\"\n",
    "    return basic_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in all_files.iterrows():\n",
    "    try:\n",
    "        prompt = build_file_granite_prompt(row)\n",
    "        chat = [\n",
    "            {\"role\": \"system\", \"content\": SHORT_FILE_SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "        # Tokenize input\n",
    "        input_tokens = tokenizer(\n",
    "            chat,\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=True\n",
    "        ).input_ids.to(runtimeFlag)\n",
    "\n",
    "        # Define token constraints\n",
    "        max_context = int(model.config.max_position_embeddings * scaling_factor)\n",
    "        max_prompt_len = min(1024, int(0.85 * max_context))  # Ensure it's within bounds\n",
    "        max_gen_len = min(1024, int(0.10 * max_context))\n",
    "\n",
    "        # Free CUDA memory before running inference\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Generate response\n",
    "        output = model.generate(input_ids=input_tokens, max_new_tokens=max_gen_len)\n",
    "        new_tokens = output[0][input_tokens.shape[-1]:]\n",
    "        response = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "        all_files.at[idx, 'refined summary2'] = response\n",
    "        print(f\"{idx} done\")\n",
    "    except RuntimeError as e1:\n",
    "        all_files.at[idx, 'refined summary2'] = 'Error Long'\n",
    "        print(idx, 'Error long')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_WRITING_SYSTEM_PROMPT = \"\"\"You are an assistant helping to explain COBOL code at the file level. Our goal is to create a comprehensive explanation for the COBOL file by recursively merging explanations of its function chunks. When generating the comprehensive explanation, follow the relationships between functions.\n",
    "\n",
    "Each chunk should be formatted as follows:\n",
    "1. '<function_name>: <explanation>'\n",
    "2. Separate the explanation of each function chunk with ---.\n",
    "\n",
    "Use the <Relationship> tag to indicate relationships between functions.\n",
    "\n",
    "The comprehensive explanation should not exceed 75 words. \n",
    "Clearly define the purpose and sequence of each function, ensuring the explanation follows the flow and dependencies between functions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paragraph_name_summary(file_path, all_paragraphs):\n",
    "    df = all_paragraphs.loc[all_paragraphs['file path']==file_path]\n",
    "    df = df.loc[(df['is section']==True)|((df['is paragraph']==True)&(df['section name'].isna()))]\n",
    "    return list(zip(df['function name'].to_list(),df['refined summary2'].to_list()))\n",
    "\n",
    "def build_file_writing_prompt(row, all_paragraphs):\n",
    "    pairs = get_paragraph_name_summary(row['file path'], all_paragraphs)\n",
    "    summary_text = ''\n",
    "    for item in pairs:\n",
    "        summary_text += f\"{item[0]}: {item[1]}\\n---\\n\"\n",
    "    relation_text = row['call relations']\n",
    "    return f\"Below are explanations of each paragraph in a COBOL file:\\n{summary_text}<Relationship>\\n{relation_text}\\n</Relationship>\\nWe are creating one comprehensive explanation for the COBOL file by recursively merging explanations of its paragraph chunks. When generating the comprehensive explanation, make sure to follow the paragraph relationship. You must briefly introduce the business purpose and functionality of the file. Generate the explanation in one short paragraph and do not use bullet points. Summary (75 words):\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in all_files.index:\n",
    "    row = all_files.loc[idx]\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": FILE_WRITING_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": build_file_writing_prompt(row, all_paragraphs)}\n",
    "    ]\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        model=gpt_model_id,\n",
    "        messages=messages\n",
    "        )\n",
    "    response = chat_completion.choices[0].message.content.replace('<Main>','').replace('</Main>','').strip()\n",
    "    all_files.at[idx, 'refined summary2'] = response\n",
    "    print(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = pd.read_csv(os.path.join('..', 'data', 'file_generated_summary.csv'), index_col=0)\n",
    "all_files['paragraphs'] = all_files['paragraphs'].apply(lambda x:x.strip(']').strip('[').strip('\"').strip('\"').replace(\"'\",\"\").split(', '))\n",
    "index = all_files.loc[(all_files['paragraphs'].apply(len)==1)&(all_files['paragraphs'].apply(lambda x:x[0])=='')].index\n",
    "all_files.loc[index,'paragraphs'] = all_files.loc[index]['paragraphs'].apply(lambda x: [])\n",
    "all_files.loc[~all_files['Linked Programs'].isna(),'Linked Programs'] = all_files.loc[~all_files['Linked Programs'].isna()]['Linked Programs'].apply(lambda x: x.replace('\"','').replace('[','').replace(']','').replace(\"'\",'').replace(' ','').split(','))\n",
    "all_files.loc[~all_files['Called Programs'].isna(),'Called Programs'] = all_files.loc[~all_files['Called Programs'].isna()]['Called Programs'].apply(lambda x: x.replace('\"','').replace(\"'\",'').replace('[','').replace(']','').replace(' ','').split(','))\n",
    "all_files.loc[~all_files['File Calls'].isna(),'File Calls'] = all_files.loc[~all_files['File Calls'].isna()]['File Calls'].apply(lambda x: x.replace('\"','').replace(\"'\",'').replace('[','').replace(']','').replace(' ','').split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the file name of the linked files if it is stored in variable\n",
    "# file\n",
    "def get_filename_from_variable(program,row):\n",
    "    variables = row['data division']\n",
    "    lines = variables.splitlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        if program+' ' in line:\n",
    "            if 'value' in line.lower():    \n",
    "                substring = line.lower().split('value')[-1]\n",
    "                substring = line[len(line)-len(substring):].strip().replace('\"','').replace(\"'\",'').strip('.').strip()\n",
    "                return substring\n",
    "    return None\n",
    "\n",
    "def find_linked_program_file_name(filename, row, all_files):\n",
    "    full_path = row['file path'].replace(row['file path'].split('/')[-1],'')+filename+'.cbl'\n",
    "    df = all_files.loc[all_files['file path'].apply(lambda x:x.lower())==full_path.lower()]\n",
    "    if len(df) == 1:\n",
    "        \n",
    "        return filename\n",
    "    new_file = get_filename_from_variable(filename,row)\n",
    "    if new_file:\n",
    "        new_file_path = row['file path'].replace(row['file path'].split('/')[-1],'')+new_file+'.cbl'\n",
    "        df = all_files.loc[all_files['file path'].apply(lambda x:x.lower())==new_file_path.lower()]\n",
    "        if len(df) == 1:\n",
    "            return new_file\n",
    "        else:\n",
    "            print(f\"ERROR cannot find summary for {filename}\")\n",
    "            #print(f\"ERROR cannot find summary for {file} while generating {row['file path']} - {row['function name']}\")\n",
    "    return -1\n",
    "\n",
    "def find_all_linked_program_file_name(row, all_files):\n",
    "    linked_programs = row['Linked Programs']\n",
    "    called_programs = row['Called Programs']\n",
    "    file_calls = row['File Calls']\n",
    "    result, files = [], []\n",
    "    files += linked_programs if not isinstance(linked_programs, float) else []\n",
    "    files += called_programs if not isinstance(called_programs, float) else []\n",
    "    files += file_calls if not isinstance(file_calls, float) else []\n",
    "    for filename in files:\n",
    "        newfilename = find_linked_program_file_name(filename, row, all_files)\n",
    "        if not newfilename == -1:\n",
    "            result.append(newfilename.lower())\n",
    "    return result\n",
    "\n",
    "def level_to_string(level):\n",
    "    if level == 0:\n",
    "        return 'Top'\n",
    "    elif level == 1:\n",
    "        return 'Second'\n",
    "    elif level == 2:\n",
    "        return 'Third'\n",
    "    elif level == 3:\n",
    "        return 'Fourth'\n",
    "    elif level == 4:\n",
    "        return 'Fifth'\n",
    "    elif level == 5:\n",
    "        return 'Sixth'\n",
    "    elif level == 6:\n",
    "        return 'Seventh'\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are a few file links cannot be find\n",
    "all_files['linked program name'] = all_files.apply(lambda x: find_all_linked_program_file_name(x, all_files), axis=1)\n",
    "all_files['filename'] = all_files['filename'].apply(lambda x:x.replace('.cbl','').lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.drawing.nx_pydot import graphviz_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the levels based on the node vertical position in the nx graph <==> topology ranking\n",
    "def get_levels(repo, all_files):\n",
    "    G = nx.DiGraph()\n",
    "    for index, row in all_files.loc[all_files['repo']==repo].iterrows():\n",
    "        name = row['filename']\n",
    "        linked_programs = row['linked program name']\n",
    "        if len(linked_programs)>0:\n",
    "            linked_programs_list = linked_programs\n",
    "            for program in linked_programs_list:\n",
    "                G.add_edge(name, program)\n",
    "\n",
    "    # Draw the graph using graphviz_layout for a hierarchical layout\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    pos = graphviz_layout(G, prog='dot')\n",
    "\n",
    "    vertical_pos = {}\n",
    "    for filename in pos:\n",
    "        if not pos[filename][1] in vertical_pos:\n",
    "            vertical_pos[pos[filename][1]] = [filename]\n",
    "        else:\n",
    "            vertical_pos[pos[filename][1]].append(filename)\n",
    "    level_order = sorted(vertical_pos.keys(), reverse=True)\n",
    "    levels = {}\n",
    "    for level in level_order:\n",
    "        levels[level_to_string(level_order.index(level))] = vertical_pos[level]\n",
    "\n",
    "    if len(levels) == 0:\n",
    "        levels['Top'] = all_files.loc[all_files['repo']==repo]['filename'].to_list()\n",
    "\n",
    "    # nx.draw(G, pos, with_labels=True, node_size=2000, node_color=\"skyblue\", font_size=8, arrows=True)\n",
    "    # plt.title('Hierarchical Graph of Names and Linked Programs')\n",
    "    # plt.show()\n",
    "    return levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import io\n",
    "# test repo\n",
    "repo = 'debinix_openjensen'\n",
    "repo = 'cicsdev_cics-banking-sample-application-cbsa'\n",
    "G = nx.DiGraph()\n",
    "for index, row in all_files.loc[all_files['repo']==repo].iterrows():\n",
    "    name = row['filename']\n",
    "    linked_programs = row['linked program name']\n",
    "    if len(linked_programs)>0:\n",
    "        linked_programs_list = linked_programs\n",
    "        for program in linked_programs_list:\n",
    "            if program == 'toolchaintest' or program == 'toolchaindisplaytest':\n",
    "                continue\n",
    "            G.add_edge(name, program)\n",
    "\n",
    "# # Draw the graph using graphviz_layout for a hierarchical layout\n",
    "p=nx.drawing.nx_pydot.to_pydot(G)\n",
    "\n",
    "p.set_graph_defaults(size='\"20,16!\"')   \n",
    "png_str = p.create_png()\n",
    "\n",
    "# Display the PNG image in the notebook\n",
    "image_stream = io.BytesIO(png_str)\n",
    "Image(image_stream.getvalue())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_system_prompt = \"\"\"You are an AI documentation assistant. Your task is to summarize the file explanations and integrate the hierarchical structure information enclosed within the <Structure> tag. The information should be displayed level by level, with each level embraced by **. The purpose of this documentation is to help developers and beginners understand the entire COBOL system.\n",
    "Each file explanation should be formatted as follows:\n",
    "1. 'file_name: explanation'\n",
    "2. Separate the explanation of each file by - - -.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_project_prompt(repo, all_files):\n",
    "    try:\n",
    "        print(repo)\n",
    "        levels = get_levels(repo, all_files)\n",
    "        level_text = ''\n",
    "        for level in levels:\n",
    "            level_text += f\"**{level} Level** {', '.join(levels[level])}\\n\"\n",
    "        summary_text = ''\n",
    "        for index, row in all_files.loc[all_files['repo']==repo].iterrows():\n",
    "            summary_text += f\"{row['filename']}: {row['refined summary2']}\"\n",
    "            summary_text += \"\\n---\\n\"\n",
    "        prompt= f\"\"\"Combine the explanations of the following COBOL files into a single cohesive summary. Highlight the relationships and differences between the files, considering the hierarchical structure.\n",
    "\n",
    "<Structure>\n",
    "{level_text}</Structure>\n",
    "\n",
    "{summary_text}\n",
    "Create a high-level summary of the project that integrates the summuries of all the files listed. Instead of explaining each file individually, provide a cohesive and comprehensive overview of the entire project in a single paragraph.\n",
    "\"\"\"\n",
    "        return prompt   \n",
    "    except:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    # Defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key = '<your own openai key'\n",
    ")\n",
    "gpt_model_id = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in project_df.iterrows():\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": project_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": build_project_prompt(row['repo'], all_files)}\n",
    "    ]\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages\n",
    "        )\n",
    "    response = chat_completion.choices[0].message.content.replace('<Main>','').replace('</Main>','').strip()\n",
    "    project_df.at[idx, 'generated summary'] = response\n",
    "    print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
