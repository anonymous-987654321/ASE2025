{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.translate.chrf_score import sentence_chrf\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from scipy.stats import mannwhitneyu\n",
    "from cliffs_delta import cliffs_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.relpath(os.path.join(os.getcwd(), '..', 'data'))\n",
    "all_paragraphs = pd.read_csv(os.path.join('..', 'data', 'function_refined_summary.csv'), index_col=0)\n",
    "all_paragraphs['called paragraphs'] = all_paragraphs['called paragraphs'].apply(lambda x:x.strip(']').strip('[').strip('\"').strip('\"').replace(\"'\",\"\").split(', '))\n",
    "index = all_paragraphs.loc[(all_paragraphs['called paragraphs'].apply(len)==1)&(all_paragraphs['called paragraphs'].apply(lambda x:x[0])=='')].index\n",
    "all_paragraphs.loc[index,'called paragraphs'] = all_paragraphs.loc[index]['called paragraphs'].apply(lambda x: [])\n",
    "all_files = pd.read_csv(os.path.join('..', 'data', 'file_generated_summary.csv'), index_col=0)\n",
    "function_reference = pd.read_csv(os.path.join('..', 'data', 'function_level_reference_dataset.csv'), index_col=0)\n",
    "file_reference = pd.read_csv(os.path.join('..', 'data', 'file_level_reference_dataset.csv'), index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup text similarity evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meteor(candidate_summary, reference_summary, rslt_str=''):\n",
    "    candidate_summaries_tokenized = [summary.split() for summary in candidate_summary]\n",
    "    reference_summaries_tokenized = [summary.split() for summary in reference_summary]\n",
    "    meteor_scores = []\n",
    "    # Compute METEOR score\n",
    "    for i in range(len(candidate_summaries_tokenized)):\n",
    "        meteor_scores.append(meteor_score([reference_summaries_tokenized[i]], candidate_summaries_tokenized[i]))\n",
    "    print(f\"Average METEOR Score {rslt_str}:\", round(np.mean(meteor_scores),3))\n",
    "    print(f\"Median METEOR Score {rslt_str}:\", round(np.median(meteor_scores),3))\n",
    "    return meteor_scores\n",
    "\n",
    "def chrf(candidate_summary, reference_summary, rslt_str=''):\n",
    "    # Compute sentence-level chrF scores\n",
    "    chrf_scores = [sentence_chrf(ref, hyp) for ref, hyp in zip(reference_summary, candidate_summary)]\n",
    "\n",
    "    # Compute the average (corpus-level) and median chrF scores\n",
    "    avg_score = np.mean(chrf_scores)\n",
    "    median_score = np.median(chrf_scores)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Average chrF Score {rslt_str}: {round(avg_score, 3)}\")\n",
    "    print(f\"Median chrF Score {rslt_str}: {round(median_score, 3)}\")\n",
    "\n",
    "    return chrf_scores\n",
    "\n",
    "def sentencebert(candidate_summary, reference_summary, rslt_str=''):\n",
    "    # Generate embeddings\n",
    "    sentenceBert_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "    cosine_scores = []\n",
    "    for i in range(len(candidate_summary)):\n",
    "        candidate_embedding = sentenceBert_model.encode(candidate_summary[i], convert_to_tensor=True).cpu()\n",
    "        reference_embedding = sentenceBert_model.encode(reference_summary[i], convert_to_tensor=True).cpu()\n",
    "        # Calculate cosine similarity\n",
    "        cosine_scores.append(list(util.pytorch_cos_sim(candidate_embedding, reference_embedding).numpy()[0])[0])\n",
    "    print(f\"Average sentenceBERT Score {rslt_str}:\", round(np.mean(cosine_scores),4))\n",
    "    print(f\"Median sentenceBERT Score {rslt_str}:\", round(np.median(cosine_scores),4))\n",
    "    return cosine_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load granite 34b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "model_id = \"ibm-granite/granite-34b-code-instruct\" \n",
    "device_map = device_map = \"auto\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=device_map\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model.eval()\n",
    "runtimeFlag = \"cuda:1\" \n",
    "cache_dir = None \n",
    "scaling_factor = 1.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_baseline_prompt(code_snippet):\n",
    "    return f\"### Instruction:\\nSummarize the following COBOL code:\\n{code_snippet}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in function_reference.index:\n",
    "    row = function_reference.loc[idx]\n",
    "    prompt = build_baseline_prompt(row['code'])\n",
    "    chat = [{ \"role\": \"user\", \"content\":  prompt}]\n",
    "    chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "    input_tokens = tokenizer(\n",
    "        chat,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=True\n",
    "    ).input_ids.to(runtimeFlag)\n",
    "\n",
    "    max_context = int(model.config.max_position_embeddings*scaling_factor)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    # max_prompt_len = int(0.85 * max_context)\n",
    "    # max_gen_len = int(0.10 * max_prompt_len)\n",
    "    max_prompt_len = 1024\n",
    "    max_gen_len = 1024\n",
    "\n",
    "    output = model.generate(input_ids=input_tokens, max_new_tokens=150)\n",
    "    new_tokens = output[0][input_tokens.shape[-1]:]\n",
    "    response = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "    function_reference.at[idx, 'baseline'] = response\n",
    "    print(idx, 'done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code processsing agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FUNCTION_CODE_SYSTEM_PROMPT = \"\"\"Now, you are assistant to help me explain the COBOL code and answer questions about the code. I will give you the COBOL code within <Code> and variable definitions within <Variable> tags. Your answers should be concise and coherent.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_paragraph_code_prompt(code_snippet, variables):\n",
    "    return f\"<Code>\\n{code_snippet}\\n<\\Code>\\n<Variable>\\n{variables}\\n<\\Variable>\\nGenerate an explanation of the above COBOL code. The explanation should be no more than 75 words.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in all_paragraphs.index:\n",
    "    row = all_paragraphs.loc[idx]\n",
    "    prompt = build_paragraph_code_prompt(row['code'],row['variables'])\n",
    "    chat = [{ \"role\": \"system\", \"content\":  FUNCTION_CODE_SYSTEM_PROMPT},\n",
    "            { \"role\": \"user\", \"content\":  prompt}]\n",
    "    chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "    input_tokens = tokenizer(\n",
    "        chat,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=True\n",
    "    ).input_ids.to(runtimeFlag)\n",
    "\n",
    "    max_context = int(model.config.max_position_embeddings*scaling_factor)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    # max_prompt_len = int(0.85 * max_context)\n",
    "    # max_gen_len = int(0.10 * max_prompt_len)\n",
    "    max_prompt_len = 1024\n",
    "    max_gen_len = 1024\n",
    "\n",
    "    output = model.generate(input_ids=input_tokens, max_new_tokens=150)\n",
    "    new_tokens = output[0][input_tokens.shape[-1]:]\n",
    "    response = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "    all_paragraphs.at[idx, 'generated summary'] = response\n",
    "    print(idx, 'done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the explanations of the reference dataset and run text similarity evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in function_reference.index:\n",
    "    row = function_reference.loc[idx]\n",
    "    prompt = build_paragraph_code_prompt(row['code'],row['variables'])\n",
    "    chat = [{ \"role\": \"system\", \"content\":  FUNCTION_CODE_SYSTEM_PROMPT},\n",
    "            { \"role\": \"user\", \"content\":  prompt}]\n",
    "    chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "    input_tokens = tokenizer(\n",
    "        chat,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=True\n",
    "    ).input_ids.to(runtimeFlag)\n",
    "\n",
    "    max_context = int(model.config.max_position_embeddings*scaling_factor)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    # max_prompt_len = int(0.85 * max_context)\n",
    "    # max_gen_len = int(0.10 * max_prompt_len)\n",
    "    max_prompt_len = 1024\n",
    "    max_gen_len = 1024\n",
    "\n",
    "    output = model.generate(input_ids=input_tokens, max_new_tokens=150)\n",
    "    new_tokens = output[0][input_tokens.shape[-1]:]\n",
    "    response = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "    function_reference.at[idx, 'refined summary2'] = response\n",
    "    print(idx, 'done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rslt1 = meteor(function_reference['baseline'], function_reference['reference data'], rslt_str='for baseline')\n",
    "rslt2 = meteor(function_reference['refined summary2'], function_reference['reference data'], rslt_str='for generated')\n",
    "print(f'METEOR pvalue {mannwhitneyu(rslt1, rslt2).pvalue}')\n",
    "print(f'METEOR effect size {cliffs_delta(rslt1, rslt2)[1]}')\n",
    "rslt1 = chrf(function_reference['baseline'], function_reference['reference data'], rslt_str='for baseline')\n",
    "rslt2 = chrf(function_reference['refined summary2'], function_reference['reference data'], rslt_str='for generated')\n",
    "print(f'chrF pvalue {mannwhitneyu(rslt1, rslt2).pvalue}')\n",
    "print(f'chrF effect size {cliffs_delta(rslt1, rslt2)[1]}')\n",
    "rslt1 = sentencebert(function_reference['baseline'].to_list(), function_reference['reference data'].to_list(), rslt_str='for baseline')\n",
    "rslt2 = sentencebert(function_reference['refined summary2'].to_list(), function_reference['reference data'].to_list(), rslt_str='for generated')\n",
    "print(f'sentenceBert pvalue {mannwhitneyu(rslt1, rslt2).pvalue}')\n",
    "print(f'sentenceBert effect size {cliffs_delta(rslt1, rslt2)[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text processing agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    # Defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key = '<your own openai key'\n",
    ")\n",
    "gpt_model_id = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_paragraphs_ready(row, all_paragraphs):\n",
    "    paragraphs = row['called paragraphs']\n",
    "    df = all_paragraphs.loc[(all_paragraphs['file path']==row['file path'])&(all_paragraphs['function name'].isin(paragraphs))]\n",
    "    if not df['refined summary2'].isna().any():\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAGRAPH_SYSTEM_PROMPT = \"\"\"You are now an writing assistant to help me simplify a paragraph containing complex terms. I will provide the main paragraph within <Main> tags and the definitions of the terms within <Term> tags. Your task is to replace the complex terms with concise descriptions to improve readability, while making as few changes as possible to the main paragraph.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_function_writing_prompt(row, paragraph_df):\n",
    "    code_snippet, called_paragraph = row['generated summary'], row['called paragraphs']\n",
    "    called_paragraph_text = ''\n",
    "    if len(called_paragraph) > 0:\n",
    "        df = paragraph_df.loc[paragraph_df['file path']==row['file path']]\n",
    "        df = df.loc[df['function name'].isin(row['called paragraphs'])]\n",
    "        called_paragraph_text = \"\"\n",
    "        for p in called_paragraph:\n",
    "            called_paragraph_text += f'{p}: {df.loc[df[\"function name\"]==p][\"refined summary2\"].values[0]}\\n'\n",
    "    return f\"\\n<Main>\\n{code_snippet}\\n</Main>\\n\\n<Term>\\n{called_paragraph_text}\\n</Term>\\nGenerate the improved main function:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_paragraphs['refined summary2'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the paragraph with no called functions, the final explanation is already generated by text processing agent, and don't need to use text processing agent\n",
    "temp_df = all_paragraphs.loc[all_paragraphs['called paragraphs'].apply(len)==0]\n",
    "all_paragraphs.loc[temp_df.index, 'refined summary2'] = all_paragraphs.loc[temp_df.index, 'generated summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unexplained_last_iter = -1 # number of paragraph not explained in last iteration, check circular dependency\n",
    "num_ready_last_iter = -1\n",
    "while True:\n",
    "    idx_paragraph_ready = all_paragraphs.loc[(all_paragraphs['refined summary2'].isna()&(all_paragraphs.apply(lambda x: check_paragraphs_ready(x, all_paragraphs), axis=1)))].index\n",
    "    print(len(idx_paragraph_ready), 'paragraphs ready to explain')\n",
    "    if len(idx_paragraph_ready) == num_ready_last_iter and len(all_paragraphs.loc[all_paragraphs['refined summary2'].isna()]) == num_unexplained_last_iter:\n",
    "        if len(idx_paragraph_ready)==0: # all explained\n",
    "            break\n",
    "        else: #circular dependency\n",
    "            all_paragraphs.at[idx_paragraph_ready[0], 'refined summary2'] = all_paragraphs.at[idx_paragraph_ready[0], 'generated summary']\n",
    "            continue\n",
    "    num_unexplained_last_iter = len(all_paragraphs.loc[all_paragraphs['refined summary2'].isna()]) \n",
    "    num_ready_last_iter = len(idx_paragraph_ready)\n",
    "    for idx in idx_paragraph_ready:\n",
    "        row = all_paragraphs.loc[idx]\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": PARAGRAPH_SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": build_function_writing_prompt(row, all_paragraphs)}\n",
    "        ]\n",
    "\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            model=gpt_model_id,\n",
    "            messages=messages\n",
    "            )\n",
    "        response = chat_completion.choices[0].message.content.replace('<Main>','').replace('</Main>','').strip()\n",
    "        all_paragraphs.at[idx, 'refined summary2'] = response\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_baseline_prompt_file(code):\n",
    "    code_lst = code.split('\\n')\n",
    "    for i, line in enumerate(code_lst):\n",
    "        if 'procedure division' in line.lower():\n",
    "            break\n",
    "    return 'Summarize the following COBOL file:\\n'+'\\n'.join(code_lst[i:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in file_reference.index:\n",
    "    try:\n",
    "        row = file_reference.loc[idx]\n",
    "        prompt = build_baseline_prompt_file(row['code'])\n",
    "        chat = [{ \"role\": \"user\", \"content\":  prompt}]\n",
    "        chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "        input_tokens = tokenizer(\n",
    "            chat,\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=True\n",
    "        ).input_ids.to(runtimeFlag)\n",
    "\n",
    "        max_context = int(model.config.max_position_embeddings*scaling_factor)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        # max_prompt_len = int(0.85 * max_context)\n",
    "        # max_gen_len = int(0.10 * max_prompt_len)\n",
    "        max_prompt_len = 1024\n",
    "        max_gen_len = 1024\n",
    "\n",
    "        output = model.generate(input_ids=input_tokens, max_new_tokens=150)\n",
    "        new_tokens = output[0][input_tokens.shape[-1]:]\n",
    "        response = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "        file_reference.at[idx, 'baseline'] = response\n",
    "        print(idx, 'done')\n",
    "    except RuntimeError as e1:\n",
    "        file_reference.at[idx, 'baseline'] = 'Error Long'\n",
    "        print(idx, 'Error long')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHORT_FILE_SYSTEM_PROMPT = 'Now, you are assistant to help me explain a COBOL file. I will give you the procedure division of the COBOL file embraced by <Code> tags. The data division of the file is given with <variable> tags. Generate a summary of the file base on the provided procedure and data divisoin. The summary should be a single cohesive paragraph with no more than 75 words that explains the basic business purpose and logic of the COBOL file. Ensure the summary is concise and cohesive.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_file_granite_prompt(row):\n",
    "    program_id = row['program id']\n",
    "    file_name = row['filename']\n",
    "    variables = row['data division']\n",
    "    code = row['procedure division']\n",
    "    basic_prompt = f\"This COBOL file's name is {file_name} and the program ID is {program_id}. Following is the procedure and data division of the file. \\n<Code>\\n{code}\\n</Code>\\n\\n<Variable>\\n{variables}\\n</Variable>\\nGenerate a very short and cohesive explanation of the COBOL file. The explanation should be no more than 75 words.\"\n",
    "    return basic_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in all_files.iterrows():\n",
    "    try:\n",
    "        prompt = build_file_granite_prompt(row)\n",
    "        chat = [\n",
    "            {\"role\": \"system\", \"content\": SHORT_FILE_SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "        # Tokenize input\n",
    "        input_tokens = tokenizer(\n",
    "            chat,\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=True\n",
    "        ).input_ids.to(runtimeFlag)\n",
    "\n",
    "        # Define token constraints\n",
    "        max_context = int(model.config.max_position_embeddings * scaling_factor)\n",
    "        max_prompt_len = min(1024, int(0.85 * max_context))  # Ensure it's within bounds\n",
    "        max_gen_len = min(1024, int(0.10 * max_context))\n",
    "\n",
    "        # Free CUDA memory before running inference\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Generate response\n",
    "        output = model.generate(input_ids=input_tokens, max_new_tokens=max_gen_len)\n",
    "        new_tokens = output[0][input_tokens.shape[-1]:]\n",
    "        response = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "        all_files.at[idx, 'refined summary2'] = response\n",
    "        print(f\"{idx} done\")\n",
    "    except RuntimeError as e1:\n",
    "        all_files.at[idx, 'refined summary2'] = 'Error Long'\n",
    "        print(idx, 'Error long')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_WRITING_SYSTEM_PROMPT = \"\"\"You are an assistant helping to explain COBOL code at the file level. Our goal is to create a comprehensive explanation for the COBOL file by recursively merging explanations of its function chunks. When generating the comprehensive explanation, follow the relationships between functions.\n",
    "\n",
    "Each chunk should be formatted as follows:\n",
    "1. '<function_name>: <explanation>'\n",
    "2. Separate the explanation of each function chunk with ---.\n",
    "\n",
    "Use the <Relationship> tag to indicate relationships between functions.\n",
    "\n",
    "The comprehensive explanation should not exceed 75 words. \n",
    "Clearly define the purpose and sequence of each function, ensuring the explanation follows the flow and dependencies between functions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paragraph_name_summary(file_path, all_paragraphs):\n",
    "    df = all_paragraphs.loc[all_paragraphs['file path']==file_path]\n",
    "    df = df.loc[(df['is section']==True)|((df['is paragraph']==True)&(df['section name'].isna()))]\n",
    "    return list(zip(df['function name'].to_list(),df['refined summary2'].to_list()))\n",
    "\n",
    "def build_file_writing_prompt(row, all_paragraphs):\n",
    "    pairs = get_paragraph_name_summary(row['file path'], all_paragraphs)\n",
    "    summary_text = ''\n",
    "    for item in pairs:\n",
    "        summary_text += f\"{item[0]}: {item[1]}\\n---\\n\"\n",
    "    relation_text = row['call relations']\n",
    "    return f\"Below are explanations of each paragraph in a COBOL file:\\n{summary_text}<Relationship>\\n{relation_text}\\n</Relationship>\\nWe are creating one comprehensive explanation for the COBOL file by recursively merging explanations of its paragraph chunks. When generating the comprehensive explanation, make sure to follow the paragraph relationship. You must briefly introduce the business purpose and functionality of the file. Generate the explanation in one short paragraph and do not use bullet points. Summary (75 words):\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in all_files.index:\n",
    "    row = all_files.loc[idx]\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": FILE_WRITING_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": build_file_writing_prompt(row, all_paragraphs)}\n",
    "    ]\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        model=gpt_model_id,\n",
    "        messages=messages\n",
    "        )\n",
    "    response = chat_completion.choices[0].message.content.replace('<Main>','').replace('</Main>','').strip()\n",
    "    all_files.at[idx, 'refined summary2'] = response\n",
    "    print(build_file_writing_prompt(row, all_paragraphs))\n",
    "    print(response)\n",
    "    print(idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
